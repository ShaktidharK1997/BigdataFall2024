{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9465ff89-9e2b-4fc5-be0b-8879532c847a",
   "metadata": {},
   "source": [
    "# Spark initialization - spark template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4d03af9-d433-4d41-a766-bcbda55253ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=<my-app-name>, master=local[*]) created by __init__ at /tmp/ipykernel_294/2910356201.py:8 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.ui.proxyBase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJUPYTERHUB_USER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/proxy/4040\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m## to setup SPARK UI\u001b[39;00m\n\u001b[1;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRAPHFRAMES_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m## graphframes in spark configuration\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sc\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=<my-app-name>, master=local[*]) created by __init__ at /tmp/ipykernel_294/2910356201.py:8 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23888d-55da-4939-9ca4-9bb95dc30b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cd742-faff-4097-a6e6-1a2003f036b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7e1af-a5ba-47a6-bdbc-c3d0e4b8e542",
   "metadata": {},
   "source": [
    "### Open Spark UI\n",
    "``` https://csgy1-6513-fall.rcnyu.org/user/<USER_NETID>/proxy/4040/jobs/  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1c435-f4b5-45ec-a046-cf66f327670c",
   "metadata": {},
   "source": [
    "## Bakery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c0ff3-9992-4855-97d2-6f38500b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bakery = spark.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"shared/data/Bakery.csv\").repartition(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb515ad9-aa0a-412b-a004-3d58f171d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bakery.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712f74a-2a58-4316-ae1d-3c901a30daaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bakery.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebd44b-5257-4765-b3d8-f1fd82e8f4d2",
   "metadata": {},
   "source": [
    "# Question 1 Answer below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3a89a-f82f-4e43-b32c-002590f017dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bakery.createOrReplaceTempView(\"df_bakery_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848ccd8-f578-4f60-a08d-eae9280941da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, hour\n",
    "df_bakery_monday_7to11 = df_bakery.filter(\n",
    "    (dayofweek('Date') == 2) & \n",
    "    (hour('Time').between(7, 11))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed35894-a18f-48ed-bd02-435fd2de4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bakery_monday_7to11.createOrReplaceTempView(\"df_bakery_monday_7to11_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186b2a3-7c05-48f3-b6bb-141e457c703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH sales_rank AS (\n",
    "    SELECT \n",
    "        Item,\n",
    "        COUNT(*) as qty,\n",
    "        'Monday' as weekday,\n",
    "        DATE(Date) as sale_date,\n",
    "        CONCAT(CAST(HOUR(Time) AS STRING), 'AM') as hour_period,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY DATE(Date), HOUR(Time)  -- Partitioning by both date and hour\n",
    "            ORDER BY COUNT(*) DESC\n",
    "        ) as rn\n",
    "    FROM df_bakery_view\n",
    "    WHERE DAYOFWEEK(Date) = 2  -- Monday\n",
    "    AND HOUR(Time) BETWEEN 7 AND 11\n",
    "    GROUP BY Item, DATE(Date), HOUR(Time)\n",
    "    )\n",
    "    SELECT \n",
    "        Item,\n",
    "        qty,\n",
    "        weekday,\n",
    "        sale_date as Date,\n",
    "        hour_period as Hour_period\n",
    "    FROM sales_rank\n",
    "    WHERE rn = 1\n",
    "    ORDER BY sale_date ASC, hour_period DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38473a-f3b5-42f3-af45-91eac1c23c21",
   "metadata": {},
   "source": [
    "# Question 2 answer below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2553e-b7ea-40b5-8357-97e6900dca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "WITH ranked_items AS (\n",
    "    SELECT \n",
    "        Item,\n",
    "        COUNT(*) as qty,\n",
    "        CASE \n",
    "            WHEN HOUR(Time) BETWEEN 6 AND 10 THEN 'Breakfast'\n",
    "            WHEN HOUR(Time) BETWEEN 11 AND 15 THEN 'Lunch'\n",
    "            ELSE 'Dinner'\n",
    "        END as Daypart,\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(Date) IN (1, 7) THEN 'Weekend'\n",
    "            ELSE 'Weekday'\n",
    "        END as DayType,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY \n",
    "                CASE \n",
    "                    WHEN HOUR(Time) BETWEEN 6 AND 10 THEN 'Breakfast'\n",
    "                    WHEN HOUR(Time) BETWEEN 11 AND 15 THEN 'Lunch'\n",
    "                    ELSE 'Dinner'\n",
    "                END,\n",
    "                CASE \n",
    "                    WHEN DAYOFWEEK(Date) IN (1, 7) THEN 'Weekend'\n",
    "                    ELSE 'Weekday'\n",
    "                END\n",
    "            ORDER BY COUNT(*) DESC\n",
    "        ) as rn\n",
    "    FROM df_bakery_view\n",
    "    GROUP BY \n",
    "        Item,\n",
    "        CASE \n",
    "            WHEN HOUR(Time) BETWEEN 6 AND 10 THEN 'Breakfast'\n",
    "            WHEN HOUR(Time) BETWEEN 11 AND 15 THEN 'Lunch'\n",
    "            ELSE 'Dinner'\n",
    "        END,\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(Date) IN (1, 7) THEN 'Weekend'\n",
    "            ELSE 'Weekday'\n",
    "        END\n",
    "),\n",
    "top_2_items AS (\n",
    "    SELECT \n",
    "        DayType,\n",
    "        Daypart,\n",
    "        CONCAT(\n",
    "            MAX(CASE WHEN rn = 1 THEN CONCAT('( ',Item, ' ') END),\n",
    "            ', ',\n",
    "            MAX(CASE WHEN rn = 2 THEN CONCAT(Item, ' )') END)\n",
    "        ) as top_items\n",
    "    FROM ranked_items\n",
    "    WHERE rn <= 2\n",
    "    GROUP BY DayType, Daypart\n",
    ")\n",
    "SELECT \n",
    "    DayType,\n",
    "    Daypart,\n",
    "    top_items\n",
    "FROM top_2_items\n",
    "ORDER BY \n",
    "    CASE WHEN DayType = 'Weekend' THEN 1 ELSE 2 END,\n",
    "    CASE \n",
    "        WHEN Daypart = 'Breakfast' THEN 1\n",
    "        WHEN Daypart = 'Lunch' THEN 2\n",
    "        ELSE 3\n",
    "    END;\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52dcf-57e3-4025-87e7-8b557c59f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bakery.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434b78-3a62-4293-9ecb-23a7bca2fe14",
   "metadata": {},
   "source": [
    "# Question 3 answer below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e645a61-753b-45d5-be79-c52866ec6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurants_in_Durham_County_NC = spark.read\\\n",
    "  .option(\"header\", True)\\\n",
    "  .option(\"inferSchema\", True)\\\n",
    "  .json(\"shared/data/Restaurants_in_Durham_County_NC.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80ee6-efac-43c9-9970-a711850a92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurants_in_Durham_County_NC.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c9226-63d7-4f0c-aa2d-cb28e0fe9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurants_in_Durham_County_NC.createOrReplaceTempView(\"Restaurants_in_Durham_County_NC_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b1621-08c5-4d2a-986c-d722686113c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "    fields.rpt_area_desc as area_desc,\n",
    "    COUNT(*) as count\n",
    "FROM Restaurants_in_Durham_County_NC_view\n",
    "WHERE fields.rpt_area_desc IS NOT NULL\n",
    "GROUP BY fields.rpt_area_desc\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee43f3-c711-4d48-ba42-af7ccfb5eb1b",
   "metadata": {},
   "source": [
    "# Question 4 answer below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6cec2-67e9-46ce-ba77-207bab5c09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV first\n",
    "from pyspark.sql.functions import col, round, max, min, lit\n",
    "\n",
    "# Read CSV with proper schema inference\n",
    "df_population = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"shared/data/populationbycountry19802010millions.csv\") \\\n",
    "    .withColumnRenamed(\"_c0\", \"Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8922c9f-74ee-4844-b779-9a43d33ef57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population.select('1990').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ae082-4cec-4e24-8a66-80a15ff1dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, isnull, round\n",
    "\n",
    "# Calculate percentage change between 1990 and 2000\n",
    "df_changes = df_population \\\n",
    "    .where(col(\"Country\") != \"World\") \\\n",
    "    .select(\n",
    "        \"Country\",\n",
    "        when(\n",
    "            (col(\"1990\").isNull()) | (col(\"2000\").isNull()) | (col(\"1990\") == 0),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            round(((col(\"2000\") - col(\"1990\")) / col(\"1990\") * 100), 2)\n",
    "        ).alias(\"pct_change\")\n",
    "    )\n",
    "            \n",
    "# Find max and min changes\n",
    "max_change = df_changes.agg(max(\"pct_change\").alias(\"max_change\")).collect()[0][\"max_change\"]\n",
    "min_change = df_changes.agg(min(\"pct_change\").alias(\"min_change\")).collect()[0][\"min_change\"]\n",
    "\n",
    "# Get countries with max and min changes\n",
    "result = df_changes \\\n",
    "    .where((col(\"pct_change\") == max_change) | (col(\"pct_change\") == min_change)) \\\n",
    "    .orderBy(col(\"pct_change\").desc())\n",
    "\n",
    "# Display results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44980f0-d499-4b98-aad0-fc35bea68945",
   "metadata": {},
   "source": [
    "# Question 5 & 6 answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028d64f-7ee8-41e9-bb35-645a577379d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, explode, lower, concat_ws, count\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05982368-e079-4f37-8fe8-11b3fd798d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_files(file_paths):\n",
    "\n",
    "    text_data = []\n",
    "    for file_path in file_paths:\n",
    "        text_df = spark.read.text(file_path)\n",
    "        text_data.append(text_df)\n",
    "    \n",
    "    return reduce(DataFrame.unionAll, text_data) if len(text_data) > 1 else text_data[0]\n",
    "\n",
    "\n",
    "def word_count_analysis(df):\n",
    "    # Convert to lowercase and clean text\n",
    "    df_cleaned = df.select(\n",
    "        lower(col(\"value\")).alias(\"text\")\n",
    "    )\n",
    "    \n",
    "    # Create RegexTokenizer to remove punctuation and tokenize\n",
    "    regexTokenizer = RegexTokenizer(\n",
    "        inputCol=\"text\", \n",
    "        outputCol=\"words\", \n",
    "        pattern=\"[^0-9a-z]\"  # Keep only alphanumeric characters\n",
    "    )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    words_df = regexTokenizer.transform(df_cleaned)\n",
    "    \n",
    "    # Explode the words array and count occurrences\n",
    "    word_counts = words_df.select(\n",
    "        explode(col(\"words\")).alias(\"word\")\n",
    "    ).filter(\n",
    "        col(\"word\") != \"\"  # Remove empty strings\n",
    "    ).groupBy(\n",
    "        \"word\"\n",
    "    ).count().orderBy(\n",
    "        col(\"count\").desc()\n",
    "    )\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def bigram_analysis(df):\n",
    "    # Clean and tokenize text as before\n",
    "    df_cleaned = df.select(lower(col(\"value\")).alias(\"text\"))\n",
    "    \n",
    "    regexTokenizer = RegexTokenizer(\n",
    "        inputCol=\"text\", \n",
    "        outputCol=\"words\", \n",
    "        pattern=\"[^0-9a-z]\"\n",
    "    )\n",
    "    \n",
    "    words_df = regexTokenizer.transform(df_cleaned)\n",
    "    \n",
    "    # Create bigrams\n",
    "    ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "    bigrams_df = ngram.transform(words_df)\n",
    "    \n",
    "    # Count bigram occurrences\n",
    "    bigram_counts = bigrams_df.select(\n",
    "        explode(col(\"bigrams\")).alias(\"bigram\")\n",
    "    ).groupBy(\n",
    "        \"bigram\"\n",
    "    ).count().orderBy(\n",
    "        col(\"count\").desc()\n",
    "    )\n",
    "    \n",
    "    return bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d399d3-72ae-44c8-9932-ecdcdc7437c4",
   "metadata": {},
   "outputs": [],
   "source": [
    " file_paths = [\"./*.txt\"]\n",
    "    \n",
    "# Read and process text files\n",
    "text_df = read_text_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167687b1-4b6e-4461-80d2-1a96ecd13bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform word count analysis\n",
    "print(\"Word Count Analysis:\")\n",
    "word_counts = word_count_analysis(text_df)\n",
    "word_counts.show(20) \n",
    "    \n",
    "# Perform bigram analysis\n",
    "print(\"\\nTop 10 Bigrams:\")\n",
    "bigram_counts = bigram_analysis(text_df)\n",
    "bigram_counts.show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b30ba-365b-420b-9e0a-e05f7ba9e331",
   "metadata": {},
   "source": [
    "# Question 7 answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f4d40-b7c7-45e6-a83e-12c257ff761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurants_in_Durham_County_NC = spark.read\\\n",
    "  .option(\"header\", True)\\\n",
    "  .option(\"inferSchema\", True)\\\n",
    "  .json(\"shared/data/Restaurants_in_Durham_County_NC.json\")\n",
    "\n",
    "foreclosures_df = spark.read.json(\"./shared/data/durham-nc-foreclosure-2006-2016.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9eb613-7770-4370-90d8-e66fb5e9c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import element_at\n",
    "Restaurants_in_Durham_County_NC.select((col('fields'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee13f9-625c-41de-8f76-d60fe75ec80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "from pyspark.sql.functions import udf, col, element_at, isnan\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Part A: Find closest restaurant\n",
    "INITIAL_LAT = 35.994914\n",
    "INITIAL_LON = -78.897133\n",
    "\n",
    "def calculate_distance(lat, lon):\n",
    "    try:\n",
    "        if lat is None or lon is None:\n",
    "            return None\n",
    "        return haversine(\n",
    "            (float(INITIAL_LAT), float(INITIAL_LON)), \n",
    "            (float(lat), float(lon)),\n",
    "            unit='mi'\n",
    "        )\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "distance_calculator_udf = udf(calculate_distance, DoubleType())\n",
    "\n",
    "# Find closest restaurant\n",
    "restaurant_distances = Restaurants_in_Durham_County_NC \\\n",
    "    .filter(\n",
    "        (col('fields.status') == 'ACTIVE') & \n",
    "        (col('fields.rpt_area_desc') == 'Food Service')\n",
    "    ) \\\n",
    "    .select(\n",
    "        col('fields.premise_name').alias('rest_name'),\n",
    "        element_at(col('fields.geolocation'), 1).alias('latitude'),\n",
    "        element_at(col('fields.geolocation'), 2).alias('longitude'),\n",
    "        distance_calculator_udf(\n",
    "            element_at(col('fields.geolocation'), 1),\n",
    "            element_at(col('fields.geolocation'), 2)\n",
    "        ).alias('distance')\n",
    "    ) \\\n",
    "    .where(col('distance').isNotNull()) \\\n",
    "    .orderBy('distance') \\\n",
    "    .limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ffb8dd-266e-403e-a7b0-2e5632a65b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Find foreclosures near the closest restaurant\n",
    "closest_restaurant = restaurant_distances.first()\n",
    "if closest_restaurant:\n",
    "    # Create new distance calculator using restaurant coordinates\n",
    "    def calculate_distance_from_restaurant(lat, lon):\n",
    "        try:\n",
    "            if lat is None or lon is None:\n",
    "                return None\n",
    "            return haversine(\n",
    "                (float(closest_restaurant['latitude']), float(closest_restaurant['longitude'])), \n",
    "                (float(lat), float(lon)),\n",
    "                unit='mi'\n",
    "            )\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    restaurant_distance_calculator_udf = udf(calculate_distance_from_restaurant, DoubleType())\n",
    "\n",
    "    # Find foreclosures within 1 mile of closest restaurant\n",
    "    foreclosed_properties_within_one_mile = durham_nc_foreclosure_2006_2016 \\\n",
    "        .filter(col('fields.geocode').isNotNull()) \\\n",
    "        .select(\n",
    "            element_at(col('fields.geocode'), 1).alias('latitude'),\n",
    "            element_at(col('fields.geocode'), 2).alias('longitude'),\n",
    "            restaurant_distance_calculator_udf(\n",
    "                element_at(col('fields.geocode'), 1),\n",
    "                element_at(col('fields.geocode'), 2)\n",
    "            ).alias('distance')\n",
    "        ) \\\n",
    "        .where(\n",
    "            col('latitude').isNotNull() & \n",
    "            col('longitude').isNotNull() & \n",
    "            col('distance').isNotNull() & \n",
    "            (col('distance') <= 1.0)\n",
    "        )\n",
    "\n",
    "    print(\"\\nClosest restaurant found:\")\n",
    "    restaurant_distances.show(truncate=False)\n",
    "    print(f\"\\nNumber of foreclosures within 1 mile of closest restaurant: {foreclosed_properties_within_one_mile.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe577df-3d3a-47d4-89f7-2a0043eed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosed_properties_within_one_mile.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf4a43-df47-4173-bcdf-a38ee6ed3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosed_properties_within_one_mile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e9482-7602-4867-b816-99ca4a0386fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
